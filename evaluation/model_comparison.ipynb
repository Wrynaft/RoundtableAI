{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison Experiment\n",
    "\n",
    "This notebook compares **efficiency metrics only** across different Gemini LLM models:\n",
    "- **gemini-2.0-flash** (Balanced - baseline)\n",
    "- **gemini-2.5-pro** (High quality, slower)\n",
    "- **gemini-2.0-flash-lite** (Optimized for speed)\n",
    "\n",
    "**Evaluation Scope**: Pure efficiency comparison (no backtesting)\n",
    "\n",
    "**Metrics Tracked**:\n",
    "- Time/Round\n",
    "- Time/Agent\n",
    "- Avg Rounds\n",
    "- Total Time\n",
    "- Consensus Rate\n",
    "\n",
    "**Important**:\n",
    "- Orchestrator uses `agent.chat()` to avoid double-invoke bug that caused token explosion\n",
    "- SDK retries are enabled (default `max_retries=2`) for graceful rate limit handling\n",
    "- Application-level retry logic ensures rate limit wait times are **excluded** from timing metrics\n",
    "- Failed debates are **excluded** from efficiency calculations\n",
    "- Token metrics are not tracked to maintain accuracy (avoiding estimation inaccuracies)\n",
    "\n",
    "**Note**: For backtesting performance metrics (Sharpe ratio, returns), use the separate `backtesting.ipynb` notebook with the full 30-stock KLCI benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Imports successful\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict\n",
    "import time\n",
    "\n",
    "# Import RoundtableAI components\n",
    "from agents.orchestrator import DebateOrchestrator\n",
    "from agents.debate_metrics import ModelComparisonMetrics\n",
    "from utils.database import get_mongo_collection\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âœ“ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 3 models on 5 stocks\n",
      "Models: gemini-2.0-flash, gemini-2.5-pro, gemini-2.0-flash-lite\n"
     ]
    }
   ],
   "source": [
    "# Models to compare\n",
    "MODELS_TO_TEST = [\n",
    "    \"gemini-2.0-flash\",       # Balanced (baseline)\n",
    "    \"gemini-2.5-pro\",         # High quality, slower\n",
    "    \"gemini-2.0-flash-lite\",  # Optimized for speed\n",
    "]\n",
    "\n",
    "# Debate parameters (same for all models for fair comparison)\n",
    "MAX_ROUNDS = 5\n",
    "CONSENSUS_THRESHOLD = 0.75\n",
    "RISK_TOLERANCE = \"moderate\"\n",
    "\n",
    "# Test configuration\n",
    "NUM_TEST_STOCKS = 5  # Sufficient for efficiency comparison\n",
    "\n",
    "# Results directory\n",
    "RESULTS_DIR = \"results\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Testing {len(MODELS_TO_TEST)} models on {NUM_TEST_STOCKS} stocks\")\n",
    "print(f\"Models: {', '.join(MODELS_TO_TEST)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load KLCI Test Stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test stocks (5):\n",
      " ticker       company_name\n",
      "1155.KL            Maybank\n",
      "1023.KL               CIMB\n",
      "5347.KL    Tenaga Nasional\n",
      "1295.KL        Public Bank\n",
      "5183.KL Petronas Chemicals\n"
     ]
    }
   ],
   "source": [
    "# Load KLCI constituents\n",
    "klci_stocks_df = pd.read_csv(\"test_dataset.csv\")\n",
    "\n",
    "# Select subset for testing\n",
    "test_stocks = klci_stocks_df.head(NUM_TEST_STOCKS)\n",
    "\n",
    "print(f\"\\nTest stocks ({len(test_stocks)}):\")\n",
    "print(test_stocks[['ticker', 'company_name']].to_string(index=False))\n",
    "\n",
    "# Prepare stock list\n",
    "stock_list = test_stocks.to_dict('records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def run_debate_with_metrics(orchestrator, company_name, ticker, max_retries=3):\n",
    "    \"\"\"\n",
    "    Run a single debate and return result with metrics.\n",
    "    \n",
    "    Implements retry logic for rate limits without counting wait time in metrics.\n",
    "    \n",
    "    Args:\n",
    "        orchestrator: DebateOrchestrator instance\n",
    "        company_name: Company name\n",
    "        ticker: Stock ticker\n",
    "        max_retries: Maximum number of retries for rate limit errors\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (result, metrics, success, error, rate_limit_hits)\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    rate_limit_hits = 0\n",
    "    last_error = None\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Run debate - timing is tracked internally\n",
    "            result = orchestrator.run_debate(company_name, risk_tolerance=RISK_TOLERANCE)\n",
    "            metrics = result.metrics if hasattr(result, 'metrics') else None\n",
    "            \n",
    "            # Return with rate limit hit count\n",
    "            return result, metrics, True, None, rate_limit_hits\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_str = str(e)\n",
    "            last_error = error_str\n",
    "            \n",
    "            # Check if this is a rate limit error\n",
    "            is_rate_limit = any(indicator in error_str.lower() for indicator in [\n",
    "                'rate limit',\n",
    "                'quota',\n",
    "                '429',\n",
    "                'resource exhausted',\n",
    "                'too many requests'\n",
    "            ])\n",
    "            \n",
    "            if is_rate_limit and attempt < max_retries:\n",
    "                rate_limit_hits += 1\n",
    "                \n",
    "                # Extract suggested wait time from error message\n",
    "                wait_time = 60  # Default wait time\n",
    "                \n",
    "                backoff_multiplier = 2 ** attempt\n",
    "                total_wait = min(wait_time * backoff_multiplier, 300)  # Cap at 5 minutes\n",
    "                \n",
    "                print(f\"  âš ï¸  Rate limit hit (attempt {attempt + 1}/{max_retries})\")\n",
    "                print(f\"  â³ Waiting {total_wait}s before retry (not counted in metrics)...\")\n",
    "                \n",
    "                # Wait WITHOUT this time being counted in metrics\n",
    "                # (because we'll create a fresh orchestrator/debate session)\n",
    "                time.sleep(total_wait)\n",
    "                \n",
    "                # Continue to next retry attempt\n",
    "                continue\n",
    "            else:\n",
    "                # Non-retryable error or max retries exceeded\n",
    "                return None, None, False, last_error, rate_limit_hits\n",
    "    \n",
    "    # Should never reach here, but just in case\n",
    "    return None, None, False, last_error, rate_limit_hits\n",
    "\n",
    "print(\"âœ“ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STARTING MODEL COMPARISON EXPERIMENT\n",
      "================================================================================\n",
      "Test Stocks: 5\n",
      "Models: 3\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "MODEL: gemini-2.0-flash\n",
      "================================================================================\n",
      "Initializing orchestrator...\n",
      "Initializing Gemini model: gemini-2.0-flash\n",
      "Successfully connected to Gemini model: gemini-2.0-flash\n",
      "\n",
      "[1/5] Maybank (1155.KL)\n"
     ]
    }
   ],
   "source": [
    "# Store all results\n",
    "all_model_results = {}\n",
    "all_model_metrics = {}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STARTING MODEL COMPARISON EXPERIMENT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Test Stocks: {len(stock_list)}\")\n",
    "print(f\"Models: {len(MODELS_TO_TEST)}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name in MODELS_TO_TEST:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"MODEL: {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Initialize orchestrator for this model\n",
    "    print(f\"Initializing orchestrator...\")\n",
    "    try:\n",
    "        orchestrator = DebateOrchestrator(\n",
    "            model_name=model_name,\n",
    "            max_rounds=MAX_ROUNDS,\n",
    "            consensus_threshold=CONSENSUS_THRESHOLD,\n",
    "            track_metrics=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to initialize {model_name}: {e}\")\n",
    "        print(f\"Skipping {model_name}...\\n\")\n",
    "        continue\n",
    "    \n",
    "    # Store results for this model\n",
    "    model_results = []\n",
    "    model_comparison = ModelComparisonMetrics(model_name=model_name)\n",
    "    \n",
    "    # Track rate limit statistics\n",
    "    total_rate_limit_hits = 0\n",
    "    \n",
    "    # Run debate for each stock\n",
    "    for idx, stock in enumerate(stock_list, 1):\n",
    "        ticker = stock['ticker']\n",
    "        company = stock['company_name']\n",
    "        \n",
    "        print(f\"\\n[{idx}/{len(stock_list)}] {company} ({ticker})\")\n",
    "        \n",
    "        # Run debate with retry logic\n",
    "        result, metrics, success, error, rate_limit_hits = run_debate_with_metrics(\n",
    "            orchestrator, company, ticker\n",
    "        )\n",
    "        \n",
    "        total_rate_limit_hits += rate_limit_hits\n",
    "        \n",
    "        if not success:\n",
    "            print(f\"  âŒ Error: {error}\")\n",
    "            model_results.append({\n",
    "                'ticker': ticker,\n",
    "                'company': company,\n",
    "                'success': False,\n",
    "                'error': error,\n",
    "                'rate_limit_hits': rate_limit_hits\n",
    "            })\n",
    "            # IMPORTANT: Do NOT add failed debates to metrics\n",
    "            continue\n",
    "        \n",
    "        # Store result\n",
    "        recommendation = result.recommendation.value\n",
    "        confidence = result.confidence\n",
    "        \n",
    "        print(f\"  Recommendation: {recommendation} (confidence: {confidence:.0%})\")\n",
    "        \n",
    "        if metrics:\n",
    "            print(f\"  Time: {metrics.total_time:.1f}s | Rounds: {metrics.rounds_completed}\")\n",
    "            if rate_limit_hits > 0:\n",
    "                print(f\"  âš ï¸  Rate limit hits: {rate_limit_hits} (wait time excluded from metrics)\")\n",
    "            \n",
    "            # Only add successful debates to efficiency metrics\n",
    "            model_comparison.add_debate(metrics)\n",
    "        \n",
    "        model_results.append({\n",
    "            'ticker': ticker,\n",
    "            'company': company,\n",
    "            'recommendation': recommendation,\n",
    "            'confidence': confidence,\n",
    "            'success': True,\n",
    "            'rate_limit_hits': rate_limit_hits,\n",
    "            'metrics': metrics.to_dict() if metrics else None\n",
    "        })\n",
    "        \n",
    "        # Small delay between stocks to avoid rate limits\n",
    "        time.sleep(1)\n",
    "    \n",
    "    # Store model results\n",
    "    all_model_results[model_name] = model_results\n",
    "    all_model_metrics[model_name] = model_comparison\n",
    "    \n",
    "    # Print model summary\n",
    "    print(f\"\\n{'-'*80}\")\n",
    "    print(f\"SUMMARY: {model_name}\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    successful = sum(1 for r in model_results if r['success'])\n",
    "    failed = sum(1 for r in model_results if not r['success'])\n",
    "    print(f\"Completed: {successful}/{len(stock_list)} stocks\")\n",
    "    if failed > 0:\n",
    "        print(f\"Failed: {failed} (excluded from efficiency metrics)\")\n",
    "    if total_rate_limit_hits > 0:\n",
    "        print(f\"Rate limit hits: {total_rate_limit_hits} (wait time excluded from metrics)\")\n",
    "    model_comparison.print_summary()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"EXPERIMENT COMPLETE\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficiency metrics comparison (only includes successful debates)\n",
    "efficiency_comparison = []\n",
    "\n",
    "for model_name, comparison in all_model_metrics.items():\n",
    "    # Get count of successful vs failed debates\n",
    "    results = all_model_results[model_name]\n",
    "    successful_count = sum(1 for r in results if r['success'])\n",
    "    failed_count = sum(1 for r in results if not r['success'])\n",
    "    rate_limit_hits = sum(r.get('rate_limit_hits', 0) for r in results)\n",
    "    \n",
    "    efficiency_comparison.append({\n",
    "        'Model': model_name,\n",
    "        'Successful': successful_count,\n",
    "        'Failed': failed_count,\n",
    "        'Rate Limit Hits': rate_limit_hits,\n",
    "        'Avg Total Time (s)': comparison.avg_total_time,\n",
    "        'Time/Round (s)': comparison.avg_time_per_round,\n",
    "        'Time/Agent (s)': comparison.avg_time_per_agent,\n",
    "        'Avg Rounds': comparison.avg_rounds,\n",
    "        'Consensus Rate': comparison.consensus_rate\n",
    "    })\n",
    "\n",
    "efficiency_df = pd.DataFrame(efficiency_comparison)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EFFICIENCY COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nNote: Metrics calculated from successful debates only.\")\n",
    "print(\"Rate limit wait times are excluded from timing metrics.\\n\")\n",
    "print(efficiency_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Efficiency Comparison Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare export data\n",
    "export_data = {\n",
    "    'experiment_date': datetime.now().isoformat(),\n",
    "    'configuration': {\n",
    "        'models': MODELS_TO_TEST,\n",
    "        'num_stocks': NUM_TEST_STOCKS,\n",
    "        'max_rounds': MAX_ROUNDS,\n",
    "        'consensus_threshold': CONSENSUS_THRESHOLD,\n",
    "        'risk_tolerance': RISK_TOLERANCE\n",
    "    },\n",
    "    'notes': {\n",
    "        'rate_limit_handling': 'Rate limit wait times are excluded from timing metrics',\n",
    "        'failed_debates': 'Failed debates are excluded from efficiency calculations',\n",
    "        'retry_logic': 'Up to 3 retries with exponential backoff for rate limit errors'\n",
    "    },\n",
    "    'efficiency_metrics': {\n",
    "        model: comp.to_dict()\n",
    "        for model, comp in all_model_metrics.items()\n",
    "    },\n",
    "    'detailed_results': all_model_results\n",
    "}\n",
    "\n",
    "# Save as JSON\n",
    "json_path = os.path.join(RESULTS_DIR, 'efficiency_comparison.json')\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(export_data, f, indent=2)\n",
    "print(f\"\\nâœ“ Saved: {json_path}\")\n",
    "\n",
    "# Save efficiency table as CSV\n",
    "csv_path = os.path.join(RESULTS_DIR, 'efficiency_comparison.csv')\n",
    "efficiency_df.to_csv(csv_path, index=False)\n",
    "print(f\"âœ“ Saved: {csv_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS SAVED SUCCESSFULLY\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nModels Tested: {len(MODELS_TO_TEST)}\")\n",
    "print(f\"Stocks Analyzed: {NUM_TEST_STOCKS}\")\n",
    "print(f\"Total Debates: {sum(len(r) for r in all_model_results.values())}\")\n",
    "\n",
    "# Calculate success/failure statistics\n",
    "total_successful = sum(sum(1 for r in results if r['success']) for results in all_model_results.values())\n",
    "total_failed = sum(sum(1 for r in results if not r['success']) for results in all_model_results.values())\n",
    "total_rate_limits = sum(sum(r.get('rate_limit_hits', 0) for r in results) for results in all_model_results.values())\n",
    "\n",
    "print(f\"Successful: {total_successful}\")\n",
    "if total_failed > 0:\n",
    "    print(f\"Failed: {total_failed} (excluded from efficiency metrics)\")\n",
    "if total_rate_limits > 0:\n",
    "    print(f\"Total Rate Limit Hits: {total_rate_limits} (wait time excluded from metrics)\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"KEY FINDINGS (Based on Successful Debates Only)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Filter to models with at least one successful debate\n",
    "valid_df = efficiency_df[efficiency_df['Successful'] > 0].copy()\n",
    "\n",
    "if len(valid_df) == 0:\n",
    "    print(\"\\nâš ï¸  No successful debates to analyze\")\n",
    "else:\n",
    "    # Fastest model (lowest time/round)\n",
    "    fastest = valid_df.loc[valid_df['Time/Round (s)'].idxmin()]\n",
    "    print(f\"\\nâš¡ FASTEST: {fastest['Model']}\")\n",
    "    print(f\"   Time/Round: {fastest['Time/Round (s)']:.2f}s\")\n",
    "    print(f\"   Success Rate: {fastest['Successful']}/{fastest['Successful'] + fastest['Failed']}\")\n",
    "\n",
    "    # Most efficient convergence (fewest rounds)\n",
    "    best_convergence = valid_df.loc[valid_df['Avg Rounds'].idxmin()]\n",
    "    print(f\"\\nðŸŽ¯ BEST CONVERGENCE: {best_convergence['Model']}\")\n",
    "    print(f\"   Avg Rounds: {best_convergence['Avg Rounds']:.1f}\")\n",
    "    print(f\"   Total Time: {best_convergence['Avg Total Time (s)']:.1f}s\")\n",
    "\n",
    "    # Overall fastest (total time)\n",
    "    overall_fastest = valid_df.loc[valid_df['Avg Total Time (s)'].idxmin()]\n",
    "    print(f\"\\nâš–ï¸ OVERALL FASTEST: {overall_fastest['Model']}\")\n",
    "    print(f\"   Total Time: {overall_fastest['Avg Total Time (s)']:.1f}s\")\n",
    "    print(f\"   Consensus Rate: {overall_fastest['Consensus Rate']:.0%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Files saved to:\", RESULTS_DIR)\n",
    "print(\"  - efficiency_comparison.json\")\n",
    "print(\"  - efficiency_comparison.csv\")\n",
    "print(\"  - efficiency_visualization.png\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nâ„¹ï¸  IMPORTANT NOTES:\")\n",
    "print(\"  â€¢ Rate limit wait times are EXCLUDED from all timing metrics\")\n",
    "print(\"  â€¢ Failed debates are EXCLUDED from efficiency calculations\")\n",
    "print(\"  â€¢ Only successful debates contribute to performance statistics\")\n",
    "print(\"\\n  For backtesting performance metrics (Sharpe ratio, returns),\")\n",
    "print(\"  run the backtesting.ipynb notebook with the full 30-stock KLCI benchmark.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Filter to models with successful debates\n",
    "plot_df = efficiency_df[efficiency_df['Successful'] > 0].copy()\n",
    "\n",
    "if len(plot_df) == 0:\n",
    "    print(\"âš ï¸  No successful debates to visualize\")\n",
    "else:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    \n",
    "    models = plot_df['Model'].tolist()\n",
    "    colors = ['#4CAF50', '#2196F3', '#FF9800'][:len(models)]\n",
    "    \n",
    "    # 1. Time/Round\n",
    "    ax = axes[0, 0]\n",
    "    ax.bar(range(len(models)), plot_df['Time/Round (s)'], color=colors)\n",
    "    ax.set_xticks(range(len(models)))\n",
    "    ax.set_xticklabels([m.replace('gemini-', '') for m in models], rotation=45, ha='right')\n",
    "    ax.set_ylabel('Seconds')\n",
    "    ax.set_title('Time per Round (Lower is Better)')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 2. Time/Agent\n",
    "    ax = axes[0, 1]\n",
    "    ax.bar(range(len(models)), plot_df['Time/Agent (s)'], color=colors)\n",
    "    ax.set_xticks(range(len(models)))\n",
    "    ax.set_xticklabels([m.replace('gemini-', '') for m in models], rotation=45, ha='right')\n",
    "    ax.set_ylabel('Seconds')\n",
    "    ax.set_title('Time per Agent Response (Lower is Better)')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 3. Consensus Rate\n",
    "    ax = axes[0, 2]\n",
    "    consensus_pct = plot_df['Consensus Rate'] * 100\n",
    "    ax.bar(range(len(models)), consensus_pct, color=colors)\n",
    "    ax.set_xticks(range(len(models)))\n",
    "    ax.set_xticklabels([m.replace('gemini-', '') for m in models], rotation=45, ha='right')\n",
    "    ax.set_ylabel('Percentage (%)')\n",
    "    ax.set_title('Consensus Rate (Higher is Better)')\n",
    "    ax.set_ylim([0, 110])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 4. Total Time\n",
    "    ax = axes[1, 0]\n",
    "    ax.bar(range(len(models)), plot_df['Avg Total Time (s)'], color=colors)\n",
    "    ax.set_xticks(range(len(models)))\n",
    "    ax.set_xticklabels([m.replace('gemini-', '') for m in models], rotation=45, ha='right')\n",
    "    ax.set_ylabel('Seconds')\n",
    "    ax.set_title('Avg Total Debate Time (Lower is Better)')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 5. Avg Rounds\n",
    "    ax = axes[1, 1]\n",
    "    ax.bar(range(len(models)), plot_df['Avg Rounds'], color=colors)\n",
    "    ax.set_xticks(range(len(models)))\n",
    "    ax.set_xticklabels([m.replace('gemini-', '') for m in models], rotation=45, ha='right')\n",
    "    ax.set_ylabel('Rounds')\n",
    "    ax.set_title('Avg Rounds to Completion')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 6. Success Rate & Rate Limit Hits\n",
    "    ax = axes[1, 2]\n",
    "    \n",
    "    # Calculate success rate percentage\n",
    "    success_rates = []\n",
    "    for _, row in plot_df.iterrows():\n",
    "        total = row['Successful'] + row['Failed']\n",
    "        success_rate = (row['Successful'] / total * 100) if total > 0 else 0\n",
    "        success_rates.append(success_rate)\n",
    "    \n",
    "    x = range(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    # Bar for success rate\n",
    "    bars1 = ax.bar([i - width/2 for i in x], success_rates, width, \n",
    "                   label='Success Rate (%)', color=colors, alpha=0.7)\n",
    "    \n",
    "    # Bar for rate limit hits\n",
    "    bars2 = ax.bar([i + width/2 for i in x], plot_df['Rate Limit Hits'], width,\n",
    "                   label='Rate Limit Hits', color='red', alpha=0.5)\n",
    "    \n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([m.replace('gemini-', '') for m in models], rotation=45, ha='right')\n",
    "    ax.set_ylabel('Count / Percentage')\n",
    "    ax.set_title('Success Rate & Rate Limit Hits')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Model Comparison: Efficiency Metrics\\n(Rate limit wait times excluded)', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save\n",
    "    viz_path = os.path.join(RESULTS_DIR, 'efficiency_visualization.png')\n",
    "    plt.savefig(viz_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"\\nâœ“ Saved visualization: {viz_path}\")\n",
    "    \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
