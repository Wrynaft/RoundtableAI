{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffef84ce",
   "metadata": {},
   "source": [
    "## Web Scraping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "359189f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime   \n",
    "\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "def parse_published_datetime(raw_text: str):\n",
    "    \"\"\"\n",
    "    Convert 'Wed, Dec 03, 2025 02:05pm' into datetime.\n",
    "    Removes trailing ' - 4 days' if present.\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove \" - X days\" suffix\n",
    "    text = raw_text.split(\" - \")[0].strip()\n",
    "\n",
    "    # KLSE Screener format examples:\n",
    "    # \"Wed, Dec 03, 2025 02:05pm\"\n",
    "    # \"Thu, Nov 28, 2024 11:10am\"\n",
    "    # Format: \"%a, %b %d, %Y %I:%M%p\"\n",
    "\n",
    "    formats = [\n",
    "        \"%a, %b %d, %Y %I:%M%p\",\n",
    "        \"%a, %b %d, %Y %I:%M %p\",   # handles space before AM/PM\n",
    "        \"%b %d, %Y %I:%M%p\",       # rare variant without weekday\n",
    "    ]\n",
    "\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            return datetime.strptime(text, fmt)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    # If all fail, return None instead of crashing\n",
    "    return None\n",
    "\n",
    "\n",
    "def scrape_article(url: str) -> dict:\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        return \"Failed to retrieve the article.\"\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    headline_node = soup.find('h2')\n",
    "    headline = headline_node.get_text(strip=True) if headline_node else \"No headline found.\"\n",
    "\n",
    "    info_node = soup.select_one(\"h2 + div\")\n",
    "     # Defaults\n",
    "    source = None\n",
    "    published = None\n",
    "\n",
    "    if info_node:\n",
    "        spans = info_node.find_all(\"span\")\n",
    "\n",
    "        # span[0] ‚Üí Source\n",
    "        if len(spans) >= 1:\n",
    "            source = spans[0].get_text(strip=True)\n",
    "\n",
    "        # span[1] ‚Üí Datetime (full string including the ‚Äú- 4 days‚Äù part)\n",
    "        if len(spans) >= 2:\n",
    "            published_raw = spans[1].get_text(strip=True)\n",
    "\n",
    "            # Remove trailing \" - X days\"\n",
    "            published = parse_published_datetime(published_raw.split(\" - \")[0].strip())\n",
    "\n",
    "    body_nodes = soup.find_all('p')\n",
    "    body_text = \"\\n\".join([p.get_text(strip=True) for p in body_nodes])\n",
    "\n",
    "    # Remove known noise lines\n",
    "    CLEAN_PATTERNS = [\n",
    "        r\"^Contact us\\s*\",\n",
    "        r\"The content is a snapshot from Publisher. Refer to the original content for accurate info. Contact us for any changes\\.\",\n",
    "        r\"Related Stocks.*\",\n",
    "        r\"Comments.*\",\n",
    "    ]\n",
    "    for pat in CLEAN_PATTERNS:\n",
    "        body_text = re.sub(pat, \"\", body_text, flags=re.IGNORECASE | re.DOTALL).strip()\n",
    "\n",
    "    return {\n",
    "        \"url\": url,\n",
    "        \"headline\": headline,\n",
    "        \"source\": source,\n",
    "        \"published\": published,\n",
    "        \"body\": body_text\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8b30681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = \"https://www.klsescreener.com\"\n",
    "LANG_SETTINGS_URL = f\"{BASE}/v2/news/lang_settings\"\n",
    "\n",
    "def set_english_only(session):\n",
    "    # disable Chinese\n",
    "    session.post(LANG_SETTINGS_URL, headers=HEADERS, data={\n",
    "        \"language\": \"zh\",\n",
    "        \"value\": \"false\"\n",
    "    })\n",
    "\n",
    "    # disable Malay\n",
    "    session.post(LANG_SETTINGS_URL, headers=HEADERS, data={\n",
    "        \"language\": \"ms\",\n",
    "        \"value\": \"false\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef721a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from langdetect import detect, LangDetectException\n",
    "\n",
    "def is_english(text: str) -> bool:\n",
    "    \"\"\"Return True if langdetect detects English, else False.\"\"\"\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "        return lang == \"en\"\n",
    "    except LangDetectException:\n",
    "        return False  # Unable to detect ‚Üí treat as not English\n",
    "\n",
    "def parse_datetime(dt_string):\n",
    "    \"\"\"Parse datetime from data-date attribute.\"\"\"\n",
    "    return datetime.strptime(dt_string, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def get_articles_last_n_days(stock_code: str, days: int = 180):\n",
    "    \"\"\"\n",
    "    Scrape articles for a stock from the last N days.\n",
    "    Default is 180 days (~6 months).\n",
    "    \"\"\"\n",
    "    session = requests.Session()\n",
    "\n",
    "    base_url = f\"https://www.klsescreener.com/v2/news/stock/{stock_code}\"\n",
    "    cutoff = datetime.now() - timedelta(days=days)\n",
    "\n",
    "    all_articles = []\n",
    "    page = 1\n",
    "    until_param = None\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # Build page URL\n",
    "        if until_param:\n",
    "            url = f\"{base_url}/{page}?until={until_param}\"\n",
    "        else:\n",
    "            url = base_url\n",
    "\n",
    "        print(f\"üîé Fetching: {url}\")\n",
    "        r = session.get(url, headers=HEADERS)\n",
    "        if r.status_code != 200:\n",
    "            print(\"‚ö†Ô∏è Failed to fetch page\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        article_blocks = soup.select(\"div.article\")\n",
    "\n",
    "        if not article_blocks:\n",
    "            print(\"No more articles found.\")\n",
    "            break\n",
    "\n",
    "        oldest_dt_in_page = None\n",
    "\n",
    "        for block in article_blocks:\n",
    "            # ---- Extract link ----\n",
    "            a = block.select_one(\"a[href^='/v2/news/view/']\")\n",
    "            if not a:\n",
    "                continue\n",
    "\n",
    "            link = \"https://www.klsescreener.com\" + a[\"href\"]\n",
    "            title = a.get_text(strip=True)\n",
    "\n",
    "            # Skip non-English titles\n",
    "            if not is_english(title):\n",
    "                # print(f\"‚è≠ Skipped non-English: {title}\")\n",
    "                continue\n",
    "\n",
    "            # ---- Extract datetime ----\n",
    "            dt_span = block.select_one(\"span[data-date]\")\n",
    "            if not dt_span:\n",
    "                continue\n",
    "\n",
    "            dt_str = dt_span[\"data-date\"]\n",
    "            dt = parse_datetime(dt_str)\n",
    "\n",
    "            # Track oldest dt in this batch\n",
    "            if oldest_dt_in_page is None or dt < oldest_dt_in_page:\n",
    "                oldest_dt_in_page = dt\n",
    "\n",
    "            # Stop if older than cutoff\n",
    "            if dt < cutoff:\n",
    "                print(f\"‚õî Older than {days} days reached. Stopping.\")\n",
    "                return all_articles\n",
    "\n",
    "            all_articles.append({\n",
    "                \"title\": title,\n",
    "                \"url\": link,\n",
    "                \"published\": dt_str\n",
    "            })\n",
    "\n",
    "        # Prepare next page\n",
    "        if oldest_dt_in_page:\n",
    "            until_param = int(oldest_dt_in_page.timestamp())\n",
    "            page += 1\n",
    "            time.sleep(0.5)  # be nice to server\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76909942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with 6 months (180 days) - you can adjust the days parameter\n",
    "df = get_articles_last_n_days(\"1155\", days=180)\n",
    "print(f\"Found {len(df)} articles for ticker 1155\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1de7945d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>published</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Firm fundamentals to bolster banks next year</td>\n",
       "      <td>https://www.klsescreener.com/v2/news/view/1631...</td>\n",
       "      <td>2025-12-04 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Opportunities aplenty for digital banks but no...</td>\n",
       "      <td>https://www.klsescreener.com/v2/news/view/1631...</td>\n",
       "      <td>2025-12-03 14:05:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Banking sector to navigate tighter liquidity i...</td>\n",
       "      <td>https://www.klsescreener.com/v2/news/view/1631...</td>\n",
       "      <td>2025-12-03 10:19:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Three sectors delivered strong Q3 performances...</td>\n",
       "      <td>https://www.klsescreener.com/v2/news/view/1631...</td>\n",
       "      <td>2025-12-03 08:01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Building materials, plantation top 3Q earnings...</td>\n",
       "      <td>https://www.klsescreener.com/v2/news/view/1630...</td>\n",
       "      <td>2025-12-02 14:18:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Foreign outflows cross RM20bil but local suppo...</td>\n",
       "      <td>https://www.klsescreener.com/v2/news/view/1630...</td>\n",
       "      <td>2025-12-02 10:04:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Is the FBM KLCI finally ready for 1,700 again?</td>\n",
       "      <td>https://www.klsescreener.com/v2/news/view/1629...</td>\n",
       "      <td>2025-11-29 09:18:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Local bourse ends easier on consolidation mode</td>\n",
       "      <td>https://www.klsescreener.com/v2/news/view/1626...</td>\n",
       "      <td>2025-11-26 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Local banks offer¬†flood relief assistance to a...</td>\n",
       "      <td>https://www.klsescreener.com/v2/news/view/1626...</td>\n",
       "      <td>2025-11-25 19:16:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Against the odds: Maybank's margins edge up de...</td>\n",
       "      <td>https://www.klsescreener.com/v2/news/view/1625...</td>\n",
       "      <td>2025-11-25 08:00:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Margin stability bolsters Maybank</td>\n",
       "      <td>https://www.klsescreener.com/v2/news/view/1625...</td>\n",
       "      <td>2025-11-25 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Analysts: Stronger 3Q2025 financial results, l...</td>\n",
       "      <td>https://www.klsescreener.com/v2/news/view/1625...</td>\n",
       "      <td>2025-11-24 17:10:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>DBS said to still be in pursuit of Alliance Ba...</td>\n",
       "      <td>https://www.klsescreener.com/v2/news/view/1625...</td>\n",
       "      <td>2025-11-24 15:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CIMB Securities lifts Maybank to 'buy' on stro...</td>\n",
       "      <td>https://www.klsescreener.com/v2/news/view/1625...</td>\n",
       "      <td>2025-11-24 09:25:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Trading ideas: Mah Sing, Ken, PPB, Insights An...</td>\n",
       "      <td>https://www.klsescreener.com/v2/news/view/1625...</td>\n",
       "      <td>2025-11-24 08:41:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0        Firm fundamentals to bolster banks next year   \n",
       "1   Opportunities aplenty for digital banks but no...   \n",
       "2   Banking sector to navigate tighter liquidity i...   \n",
       "3   Three sectors delivered strong Q3 performances...   \n",
       "4   Building materials, plantation top 3Q earnings...   \n",
       "5   Foreign outflows cross RM20bil but local suppo...   \n",
       "6      Is the FBM KLCI finally ready for 1,700 again?   \n",
       "7      Local bourse ends easier on consolidation mode   \n",
       "8   Local banks offer¬†flood relief assistance to a...   \n",
       "9   Against the odds: Maybank's margins edge up de...   \n",
       "10                  Margin stability bolsters Maybank   \n",
       "11  Analysts: Stronger 3Q2025 financial results, l...   \n",
       "12  DBS said to still be in pursuit of Alliance Ba...   \n",
       "13  CIMB Securities lifts Maybank to 'buy' on stro...   \n",
       "14  Trading ideas: Mah Sing, Ken, PPB, Insights An...   \n",
       "\n",
       "                                                  url            published  \n",
       "0   https://www.klsescreener.com/v2/news/view/1631...  2025-12-04 00:00:00  \n",
       "1   https://www.klsescreener.com/v2/news/view/1631...  2025-12-03 14:05:00  \n",
       "2   https://www.klsescreener.com/v2/news/view/1631...  2025-12-03 10:19:14  \n",
       "3   https://www.klsescreener.com/v2/news/view/1631...  2025-12-03 08:01:08  \n",
       "4   https://www.klsescreener.com/v2/news/view/1630...  2025-12-02 14:18:33  \n",
       "5   https://www.klsescreener.com/v2/news/view/1630...  2025-12-02 10:04:13  \n",
       "6   https://www.klsescreener.com/v2/news/view/1629...  2025-11-29 09:18:23  \n",
       "7   https://www.klsescreener.com/v2/news/view/1626...  2025-11-26 00:00:00  \n",
       "8   https://www.klsescreener.com/v2/news/view/1626...  2025-11-25 19:16:59  \n",
       "9   https://www.klsescreener.com/v2/news/view/1625...  2025-11-25 08:00:14  \n",
       "10  https://www.klsescreener.com/v2/news/view/1625...  2025-11-25 00:00:00  \n",
       "11  https://www.klsescreener.com/v2/news/view/1625...  2025-11-24 17:10:14  \n",
       "12  https://www.klsescreener.com/v2/news/view/1625...  2025-11-24 15:30:00  \n",
       "13  https://www.klsescreener.com/v2/news/view/1625...  2025-11-24 09:25:04  \n",
       "14  https://www.klsescreener.com/v2/news/view/1625...  2025-11-24 08:41:00  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a02a6cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Banking sector to navigate tighter liquidity i...\n",
      "Name: headline, dtype: object\n",
      "Body:\n",
      " KUALA LUMPUR (Dec 3): The Malaysian banking sector is poised for tightening liquidity and heightened deposit competition in 2026, after a strong finish this year.\n",
      "MBSB Research maintained a positive outlook on the sector, citing strong fundamentals and attractive dividends as key drivers to a solid 4Q2025.\n",
      "\"Bolstered by multiple tailwinds, the banking sector is in a good place ‚Äî so we expect share prices to continue their uptrend,\" MBSB said in a note on Wednesday.\n",
      "According to the firm‚Äôs note, elevated dividend yields, improving loan growth, stable net interest margins (NIMs), stronger fee income, and further recoveries in gross impaired loans (GIL) are expected to support continued share price growth.\n",
      "Although potential headwinds like asset quality and liquidity pressures persist, they are currently seen as secondary concerns.\n",
      "Citing takeaways from \"multiple banks\" on the tightening liquidity situation, MBSB expects weaker liquidity indicators in the coming quarters.\n",
      "\"We are wary that brighter loan growth prospects in CY2026 could dry out liquidity a lot faster than expected, leading to intense deposit competition returning,\" said MBSB.\n",
      "\"...Tariff impact may have a discernible impact on borrowers‚Äô financials by 2HCY2026 (as inventory stockpiles need time to deplete). But for now, asset quality issues are largely idiosyncratic and don‚Äôt hint towards more insidious trends,\" said the house.\n",
      "MBSB‚Äôs top picks in the sector are RHB Bank Bhd (KL:RHBBANK)('buy'; target price/TP: RM7.95) and Public Bank Bhd (KL:PBBANK) ('buy'; TP: RM5.05).\n",
      "Separately, Kenanga, which kept its 'overweight'¬†call on the sector, said that in a climate where growth prospects may be tested, greater credit should be given to banks that continue deploying capital efficiently.\n",
      "Accordingly, its top picks are AMMB Holdings Bhd (KL:AMBANK) ('outperform'; TP: RM6.90) for its rising earnings growth and return-on-equity (ROE) potential, with a growing trajectory to its dividend payout which distinguishes it from other high yielders.\n",
      "Meanwhile, Malayan Banking Bhd (KL:MAYBANK) ('outperform'; TP: RM11.30) serves as Kenanga‚Äôs large-cap top pick, supported by its leading market share which allows better scale and product manoeuvrability, as well as better-than-industry GIL¬†and presently stable dividend yield (circa 6%).\n",
      "Within the non-bank financial institution (NBFI) space, Kenanga highlighted Syarikat Takaful Malaysia Keluarga Bhd (KL:TAKAFUL) ('outperform'; TP: RM4.40) for its continued growth in its key credit-related products, backed by its renewed bancatakaful partnership with RHB Bank. Its increased payout expectations (6% yield) could also attract yield-seeking shariah investors.\n",
      "\"Notably, during the reporting season, we had downgraded CIMB Group Holdings Bhd (KL:CIMB) to 'market perform'.\n",
      "\"While its capital return plan was viewed favourably with special dividends raising yield prospects, we opine it could be a signal of more conservative capital deployment to meet its longer-term Forward30 growth targets,\" it added.\n"
     ]
    }
   ],
   "source": [
    "article = scrape_article(\"https://www.klsescreener.com/v2/news/view/1631309/banking-sector-to-navigate-tighter-liquidity-in-2026-after-strong-finish-this-year-analysts\")\n",
    "df = pd.DataFrame([article])\n",
    "print(df['headline'])\n",
    "print(\"Body:\\n\", article[\"body\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded88633",
   "metadata": {},
   "source": [
    "## Data Archiving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d02a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from urllib.parse import quote_plus \n",
    "from datetime import timezone\n",
    "from tqdm import tqdm\n",
    "# Connect to MongoDB\n",
    "username = quote_plus(\"Wrynaft\")\n",
    "password = quote_plus(\"Ryan@120104\")\n",
    "\n",
    "client = MongoClient(f\"mongodb+srv://{username}:{password}@cluster0.bjjt9fa.mongodb.net/?appName=Cluster0\")\n",
    "db = client['roundtable_ai']\n",
    "print(\"Connected to MongoDB\")\n",
    "\n",
    "col = db[\"articles\"]\n",
    "\n",
    "def store_articles_for_all_tickers(df, days: int = 180):\n",
    "    \"\"\"\n",
    "    Loop through df['ticker'], scrape each ticker's articles for the last N days,\n",
    "    and store them into MongoDB with upsert on URL.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'ticker' column\n",
    "        days: Number of days to look back (default 180 = ~6 months)\n",
    "    \"\"\"\n",
    "    tickers_with_no_articles = []\n",
    "    \n",
    "    for ticker in tqdm(df['ticker'], desc=\"Processing tickers\", unit=\"ticker\"):\n",
    "        ticker = str(ticker).upper().replace(\".KL\", \"\")\n",
    "        print(f\"\\n===============================\")\n",
    "        print(f\"üìå Processing ticker: {ticker}\")\n",
    "        print(f\"===============================\\n\")\n",
    "\n",
    "        # Step 1: Get all English article links (last N days)\n",
    "        links = get_articles_last_n_days(str(ticker), days=days)\n",
    "\n",
    "        print(f\"üîó Found {len(links)} article links\")\n",
    "\n",
    "        if len(links) == 0:\n",
    "            print(f\"‚ö†Ô∏è No English articles found for ticker {ticker}\")\n",
    "            tickers_with_no_articles.append(ticker)\n",
    "            continue  # Skip scraping step\n",
    "\n",
    "        # Step 2: Scrape each article\n",
    "        for item in links:\n",
    "            url = item[\"url\"]\n",
    "\n",
    "            print(f\"üì∞ Scraping article: {url}\")\n",
    "\n",
    "            article_data = scrape_article(url)\n",
    "            if isinstance(article_data, str):\n",
    "                print(f\"‚ö†Ô∏è Error: {article_data}\")\n",
    "                continue\n",
    "\n",
    "            # Step 3: Build MongoDB document\n",
    "            doc = {\n",
    "                \"ticker\": str(ticker),\n",
    "                \"url\": article_data[\"url\"],\n",
    "                \"headline\": article_data[\"headline\"],\n",
    "                \"source\": article_data[\"source\"],\n",
    "                \"published\": article_data[\"published\"],     # datetime object\n",
    "                \"body\": article_data[\"body\"],\n",
    "                \"scraped_at\": datetime.now(timezone.utc)\n",
    "            }\n",
    "\n",
    "            # Step 4: Insert or update (avoid duplicates)\n",
    "            result = col.update_one(\n",
    "                {\"url\": article_data[\"url\"],\"ticker\": str(ticker)},   # unique key\n",
    "                {\"$set\": doc},                  # update data if exists\n",
    "                upsert=True\n",
    "            )\n",
    "\n",
    "            if result.upserted_id:\n",
    "                print(f\"‚úÖ Stored new article.\")\n",
    "            else:\n",
    "                print(f\"‚ôªÔ∏è Article already exists. Updated instead.\")\n",
    "\n",
    "            time.sleep(0.3)  # be gentle to server\n",
    "\n",
    "        print(f\"‚úî Completed ticker {ticker}\\n\")\n",
    "        time.sleep(1.0)  # small delay between tickers\n",
    "\n",
    "    print(\"\\n===============================\")\n",
    "    print(\"üì¢ SUMMARY: Tickers with 0 articles\")\n",
    "    print(\"===============================\\n\")\n",
    "\n",
    "    if len(tickers_with_no_articles) == 0:\n",
    "        print(\"üéâ All tickers had at least one English article!\")\n",
    "    else:\n",
    "        for t in tickers_with_no_articles:\n",
    "            print(f\"‚ùå {t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b459ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape articles for the last 6 months (180 days)\n",
    "# Existing articles will be updated (not duplicated) thanks to upsert\n",
    "ticker_list = pd.read_csv(\"ticker_list.csv\")  # assumes a 'ticker' column\n",
    "store_articles_for_all_tickers(ticker_list, days=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "59cc3e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total tickers with at least 1 article: 390\n"
     ]
    }
   ],
   "source": [
    "tickers_with_articles = col.aggregate([\n",
    "    {\"$group\": {\"_id\": \"$ticker\", \"count\": {\"$sum\": 1}}},\n",
    "    {\"$sort\": {\"_id\": 1}}\n",
    "])\n",
    "\n",
    "# Convert cursor to list so we can iterate twice\n",
    "ticker_list = list(tickers_with_articles)\n",
    "\n",
    "for t in tickers_with_articles:\n",
    "    print(t[\"_id\"], \"->\", t[\"count\"], \"articles\")\n",
    "\n",
    "# Print total number of tickers\n",
    "print(\"\\nTotal tickers with at least 1 article:\", len(ticker_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7527535b",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39071e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ryan Chin\\Documents\\UM CS\\WIH3001 Data Science Project\\RoundtableAI\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Ryan Chin\\Documents\\UM CS\\WIH3001 Data Science Project\\RoundtableAI\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Ryan Chin\\.cache\\huggingface\\hub\\models--ProsusAI--finbert. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "\n",
    "model_name = \"ProsusAI/finbert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "LABELS = [\"positive\", \"negative\", \"neutral\"]  # FinBERT order is fixed\n",
    "\n",
    "def analyze_sentiment(text: str) -> str:\n",
    "    try:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits[0].cpu().numpy()\n",
    "        \n",
    "        probs = torch.softmax(torch.tensor(logits), dim=0).numpy()\n",
    "        max_index = int(np.argmax(probs))\n",
    "        sentiment_score = float(probs[0]-probs[1])  # positive - negative\n",
    "\n",
    "        return {\n",
    "            \"label\": LABELS[max_index],\n",
    "            \"score\": sentiment_score,\n",
    "            \"confidence\": {\n",
    "                \"positive\": float(probs[0]),\n",
    "                \"negative\": float(probs[1]),\n",
    "                \"neutral\": float(probs[2])\n",
    "            }\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52017c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to MongoDB\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "# Connect to MongoDB\n",
    "username = quote_plus(\"Wrynaft\")\n",
    "password = quote_plus(\"Ryan@120104\")\n",
    "\n",
    "client = MongoClient(f\"mongodb+srv://{username}:{password}@cluster0.bjjt9fa.mongodb.net/?appName=Cluster0\")\n",
    "db = client['roundtable_ai']\n",
    "print(\"Connected to MongoDB\")\n",
    "\n",
    "col = db[\"articles\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06b42abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2648 articles for sentiment analysis...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing articles: 2648it [01:32, 28.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis completed and stored in MongoDB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "cursor = col.find({\"sentiment\": {\"$exists\": False}})\n",
    "count = col.count_documents({\"sentiment\": {\"$exists\": False}})\n",
    "\n",
    "print(f\"Processing {count} articles for sentiment analysis...\\n\")\n",
    "\n",
    "for doc in tqdm(cursor, desc=\"Analyzing articles\"):\n",
    "    article_id = doc[\"_id\"]\n",
    "\n",
    "    headline = doc.get(\"headline\", \"\") or \"\"\n",
    "    body = doc.get(\"body\", \"\") or \"\"\n",
    "\n",
    "    full_text = headline + \"\\n\" + body\n",
    "\n",
    "    sentiment = analyze_sentiment(full_text)\n",
    "\n",
    "    col.update_one(\n",
    "        {\"_id\": article_id},\n",
    "        {\"$set\": {\"sentiment\": sentiment}}\n",
    "    )\n",
    "\n",
    "print(\"Sentiment analysis completed and stored in MongoDB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d44ac0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
