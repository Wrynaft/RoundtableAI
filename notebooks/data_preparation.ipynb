{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation - RoundtableAI\n",
    "\n",
    "**Project:** LLM-Based Multi-Agent System for Bursa Stock Analysis\n",
    "\n",
    "**Author:** Ryan Chin Jian Hwa\n",
    "\n",
    "**Course:** WIH3001 Data Science Project\n",
    "\n",
    "---\n",
    "\n",
    "## CRISP-DM: Data Preparation Phase\n",
    "\n",
    "This notebook follows the CRISP-DM methodology for data preparation:\n",
    "\n",
    "1. **Data Selection** - Select relevant data from MongoDB collections\n",
    "2. **Data Cleaning** - Handle missing values, duplicates, outliers\n",
    "3. **Data Construction** - Derive new attributes if needed\n",
    "4. **Data Integration** - Merge data from different collections\n",
    "5. **Data Formatting** - Ensure consistent formats\n",
    "\n",
    "## Collections to Prepare\n",
    "- `fundamentals` - Financial metrics (PE ratio, ROE, etc.)\n",
    "- `stock_prices` - Historical price data\n",
    "- `articles` - News articles with sentiment scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nfrom pathlib import Path\n\n# Add project root to path\nproject_root = Path.cwd().parent\nif str(project_root) not in sys.path:\n    sys.path.insert(0, str(project_root))\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime, timedelta\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# MongoDB connection using existing database utility\nfrom utils.database import get_mongo_client, get_mongo_database\n\n# Connect to MongoDB using the configured client\nclient = get_mongo_client()\ndb = get_mongo_database('bursa_stocks')  # Using your actual database name\n\nprint(f\"Connected to MongoDB\")\nprint(f\"Database: {db.name}\")\nprint(f\"Collections: {db.list_collection_names()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Inspection - Fundamentals Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fundamentals data\n",
    "fundamentals_cursor = db.fundamentals.find()\n",
    "fundamentals_list = list(fundamentals_cursor)\n",
    "\n",
    "print(f\"Total documents in fundamentals: {len(fundamentals_list)}\")\n",
    "print(f\"\\nSample document structure:\")\n",
    "if fundamentals_list:\n",
    "    import pprint\n",
    "    pprint.pprint(fundamentals_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Convert to DataFrame for easier analysis\nfundamentals_df = pd.DataFrame(fundamentals_list)\n\nprint(\"Fundamentals DataFrame Info:\")\nprint(f\"Shape: {fundamentals_df.shape}\")\nprint(f\"\\nColumns: {fundamentals_df.columns.tolist()}\")\nprint(f\"\\nData types:\")\nprint(fundamentals_df.dtypes)\n\n# Identify nested fields\nnested_fields = []\nfor col in fundamentals_df.columns:\n    if fundamentals_df[col].dtype == 'object':\n        # Check if it's a dict\n        sample = fundamentals_df[col].dropna().iloc[0] if len(fundamentals_df[col].dropna()) > 0 else None\n        if isinstance(sample, dict):\n            nested_fields.append(col)\n\nprint(f\"\\nNested fields detected: {nested_fields}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.1 Flatten Nested Fields for Analysis"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Flatten nested fields into separate columns\ndef flatten_nested_fields(df, nested_cols):\n    \"\"\"Flatten nested dictionary columns into separate columns.\"\"\"\n    df_flat = df.copy()\n    \n    for col in nested_cols:\n        # Extract all keys from the nested dictionaries\n        all_keys = set()\n        for item in df_flat[col].dropna():\n            if isinstance(item, dict):\n                all_keys.update(item.keys())\n        \n        # Create new columns for each key\n        for key in all_keys:\n            new_col_name = f\"{col}.{key}\"\n            df_flat[new_col_name] = df_flat[col].apply(\n                lambda x: x.get(key) if isinstance(x, dict) else None\n            )\n        \n        # Drop the original nested column\n        df_flat = df_flat.drop(columns=[col])\n    \n    return df_flat\n\n# Flatten the DataFrame\nif nested_fields:\n    print(f\"Flattening {len(nested_fields)} nested fields...\")\n    fundamentals_flat = flatten_nested_fields(fundamentals_df, nested_fields)\n    print(f\"✓ Flattened DataFrame shape: {fundamentals_flat.shape}\")\n    print(f\"\\\\nNew columns created:\")\n    \n    # Show new flattened columns\n    new_cols = [col for col in fundamentals_flat.columns if '.' in col]\n    for field in nested_fields:\n        field_cols = [col for col in new_cols if col.startswith(f\"{field}.\")]\n        if field_cols:\n            print(f\"  {field}: {len(field_cols)} sub-fields\")\n            print(f\"    {', '.join(field_cols[:10])}\")  # Show first 10\n            if len(field_cols) > 10:\n                print(f\"    ... and {len(field_cols) - 10} more\")\nelse:\n    fundamentals_flat = fundamentals_df\n    print(\"No nested fields to flatten\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.2 Missing Values Analysis - Fundamentals (Flattened)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze missing values in FLATTENED DataFrame\nmissing_analysis = pd.DataFrame({\n    'Column': fundamentals_flat.columns,\n    'Missing_Count': fundamentals_flat.isnull().sum(),\n    'Missing_Percentage': (fundamentals_flat.isnull().sum() / len(fundamentals_flat) * 100).round(2),\n    'Data_Type': fundamentals_flat.dtypes\n}).sort_values('Missing_Percentage', ascending=False)\n\nprint(\"Missing Values Analysis - Fundamentals (with nested fields flattened):\")\nprint(\"=\"*70)\ndisplay(missing_analysis.head(30))  # Show top 30 columns with most missing data\n\n# Show summary by category\nprint(\"\\\\nMissing data summary by category:\")\nprint(f\"  Columns with 100% missing (remove candidates): {len(missing_analysis[missing_analysis['Missing_Percentage'] == 100])}\")\nprint(f\"  Columns with 50-99% missing (caution): {len(missing_analysis[(missing_analysis['Missing_Percentage'] >= 50) & (missing_analysis['Missing_Percentage'] < 100)])}\")\nprint(f\"  Columns with 1-49% missing (imputable): {len(missing_analysis[(missing_analysis['Missing_Percentage'] > 0) & (missing_analysis['Missing_Percentage'] < 50)])}\")\nprint(f\"  Columns with 0% missing (complete): {len(missing_analysis[missing_analysis['Missing_Percentage'] == 0])}\")\n\n# Visualize missing data\nfig, ax = plt.subplots(figsize=(12, 8))\nmissing_pct = missing_analysis[missing_analysis['Missing_Percentage'] > 0].head(30)\nif not missing_pct.empty:\n    missing_pct.plot(x='Column', y='Missing_Percentage', kind='barh', ax=ax, color='coral')\n    ax.set_xlabel('Missing Percentage (%)')\n    ax.set_title('Top 30 Columns with Missing Values - Fundamentals')\n    ax.axvline(x=50, color='red', linestyle='--', label='50% threshold', linewidth=2)\n    ax.axvline(x=100, color='darkred', linestyle='--', label='100% missing', linewidth=2)\n    ax.legend()\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"\\\\n✓ No missing values found!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.3 Duplicate Analysis - Fundamentals"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Identify numerical columns from FLATTENED DataFrame\nnumerical_cols = fundamentals_flat.select_dtypes(include=[np.number]).columns.tolist()\nif '_id' in numerical_cols:\n    numerical_cols.remove('_id')\n\nprint(f\"Numerical columns found in flattened data: {len(numerical_cols)}\")\n\n# Show numerical columns by category\nmetrics_cols = [col for col in numerical_cols if col.startswith('metrics.')]\nincome_cols = [col for col in numerical_cols if 'income' in col.lower()]\nbalance_cols = [col for col in numerical_cols if 'balance' in col.lower()]\ncashflow_cols = [col for col in numerical_cols if 'cashflow' in col.lower()]\nother_cols = [col for col in numerical_cols if col not in metrics_cols + income_cols + balance_cols + cashflow_cols]\n\nprint(f\"\\nNumerical columns by category:\")\nprint(f\"  Metrics fields: {len(metrics_cols)}\")\nprint(f\"  Income statement: {len(income_cols)}\")\nprint(f\"  Balance sheet: {len(balance_cols)}\")\nprint(f\"  Cash flow: {len(cashflow_cols)}\")\nprint(f\"  Other: {len(other_cols)}\")\n\n# Statistical summary for key metrics (showing first 20)\nif metrics_cols:\n    print(f\"\\nKey metrics summary (showing first 20):\")\n    display(fundamentals_flat[metrics_cols[:20]].describe())"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check for outliers using IQR method on FLATTENED DataFrame\ndef detect_outliers_iqr(df, columns):\n    \"\"\"Detect outliers using IQR method.\"\"\"\n    outlier_summary = []\n    \n    for col in columns:\n        # Skip columns with all NaN\n        if df[col].isna().all():\n            continue\n            \n        Q1 = df[col].quantile(0.25)\n        Q3 = df[col].quantile(0.75)\n        IQR = Q3 - Q1\n        \n        if IQR == 0:  # Skip if no variation\n            continue\n        \n        lower_bound = Q1 - 1.5 * IQR\n        upper_bound = Q3 + 1.5 * IQR\n        \n        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]\n        \n        outlier_summary.append({\n            'Column': col,\n            'Outlier_Count': len(outliers),\n            'Outlier_Percentage': round(len(outliers) / len(df.dropna(subset=[col])) * 100, 2) if len(df.dropna(subset=[col])) > 0 else 0,\n            'Lower_Bound': round(lower_bound, 2),\n            'Upper_Bound': round(upper_bound, 2),\n            'Min': round(df[col].min(), 2),\n            'Max': round(df[col].max(), 2)\n        })\n    \n    return pd.DataFrame(outlier_summary).sort_values('Outlier_Percentage', ascending=False)\n\nif numerical_cols:\n    print(\"Detecting outliers in numerical columns (this may take a moment)...\")\n    outliers_df = detect_outliers_iqr(fundamentals_flat, numerical_cols)\n    \n    print(f\"\\nOutlier Analysis (IQR Method) - Top 20 columns:\")\n    print(\"=\"*70)\n    display(outliers_df.head(20))\n    \n    # Summary\n    high_outliers = len(outliers_df[outliers_df['Outlier_Percentage'] > 10])\n    print(f\"\\nColumns with >10% outliers: {high_outliers}\")\nelse:\n    print(\"No numerical columns to analyze\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Data Inspection - Stock Prices Collection"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load stock prices data\nstock_prices_cursor = db.stock_prices.find()\nstock_prices_list = list(stock_prices_cursor)\n\nprint(f\"Total documents in stock_prices: {len(stock_prices_list)}\")\nprint(f\"\\nSample document structure:\")\nif stock_prices_list:\n    pprint.pprint(stock_prices_list[0])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "stock_prices_df = pd.DataFrame(stock_prices_list)\n",
    "\n",
    "print(\"Stock Prices DataFrame Info:\")\n",
    "print(f\"Shape: {stock_prices_df.shape}\")\n",
    "print(f\"\\nColumns: {stock_prices_df.columns.tolist()}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(stock_prices_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Date Range and Consistency - Stock Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze date column\n",
    "date_col = None\n",
    "for col in ['date', 'timestamp', 'datetime', 'Date']:\n",
    "    if col in stock_prices_df.columns:\n",
    "        date_col = col\n",
    "        break\n",
    "\n",
    "if date_col:\n",
    "    # Convert to datetime if needed\n",
    "    if stock_prices_df[date_col].dtype == 'object':\n",
    "        stock_prices_df[date_col] = pd.to_datetime(stock_prices_df[date_col], errors='coerce')\n",
    "    \n",
    "    print(f\"Date column: {date_col}\")\n",
    "    print(f\"Date range: {stock_prices_df[date_col].min()} to {stock_prices_df[date_col].max()}\")\n",
    "    print(f\"Total days covered: {(stock_prices_df[date_col].max() - stock_prices_df[date_col].min()).days}\")\n",
    "    print(f\"\\nDate data type: {stock_prices_df[date_col].dtype}\")\n",
    "    print(f\"Invalid dates: {stock_prices_df[date_col].isna().sum()}\")\n",
    "else:\n",
    "    print(\"Warning: No date column found in stock_prices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Missing Values and Data Quality - Stock Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values analysis\n",
    "missing_prices = pd.DataFrame({\n",
    "    'Column': stock_prices_df.columns,\n",
    "    'Missing_Count': stock_prices_df.isnull().sum(),\n",
    "    'Missing_Percentage': (stock_prices_df.isnull().sum() / len(stock_prices_df) * 100).round(2),\n",
    "    'Data_Type': stock_prices_df.dtypes\n",
    "}).sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "print(\"Missing Values Analysis - Stock Prices:\")\n",
    "print(\"=\"*70)\n",
    "display(missing_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data completeness by ticker\n",
    "if 'ticker' in stock_prices_df.columns:\n",
    "    ticker_stats = stock_prices_df.groupby('ticker').agg({\n",
    "        date_col if date_col else stock_prices_df.columns[0]: ['count', 'min', 'max']\n",
    "    }).reset_index()\n",
    "    \n",
    "    ticker_stats.columns = ['ticker', 'record_count', 'earliest_date', 'latest_date']\n",
    "    \n",
    "    print(\"\\nData completeness by ticker:\")\n",
    "    print(f\"Total unique tickers: {ticker_stats['ticker'].nunique()}\")\n",
    "    print(f\"\\nRecord count statistics:\")\n",
    "    print(ticker_stats['record_count'].describe())\n",
    "    \n",
    "    print(\"\\nTickers with fewest records:\")\n",
    "    display(ticker_stats.nsmallest(10, 'record_count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Price Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for negative or zero prices\n",
    "price_cols = ['open', 'high', 'low', 'close', 'Open', 'High', 'Low', 'Close']\n",
    "price_cols = [col for col in price_cols if col in stock_prices_df.columns]\n",
    "\n",
    "if price_cols:\n",
    "    print(\"Price validation checks:\")\n",
    "    for col in price_cols:\n",
    "        negative = (stock_prices_df[col] < 0).sum()\n",
    "        zero = (stock_prices_df[col] == 0).sum()\n",
    "        print(f\"  {col}: {negative} negative values, {zero} zero values\")\n",
    "    \n",
    "    # Check High >= Low\n",
    "    if 'high' in price_cols and 'low' in price_cols:\n",
    "        invalid_range = (stock_prices_df['high'] < stock_prices_df['low']).sum()\n",
    "        print(f\"\\nInvalid price ranges (High < Low): {invalid_range}\")\n",
    "    elif 'High' in price_cols and 'Low' in price_cols:\n",
    "        invalid_range = (stock_prices_df['High'] < stock_prices_df['Low']).sum()\n",
    "        print(f\"\\nInvalid price ranges (High < Low): {invalid_range}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Inspection - Articles Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load articles data\n",
    "articles_cursor = db.articles.find()\n",
    "articles_list = list(articles_cursor)\n",
    "\n",
    "print(f\"Total documents in articles: {len(articles_list)}\")\n",
    "print(f\"\\nSample document structure:\")\n",
    "if articles_list:\n",
    "    pprint.pprint(articles_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Convert to DataFrame\narticles_df = pd.DataFrame(articles_list)\n\nprint(\"Articles DataFrame Info:\")\nprint(f\"Shape: {articles_df.shape}\")\nprint(f\"\\nColumns: {articles_df.columns.tolist()}\")\nprint(f\"\\nData types:\")\nprint(articles_df.dtypes)\n\n# Check for nested fields in articles\narticles_nested_fields = []\nfor col in articles_df.columns:\n    if articles_df[col].dtype == 'object':\n        sample = articles_df[col].dropna().iloc[0] if len(articles_df[col].dropna()) > 0 else None\n        if isinstance(sample, dict):\n            articles_nested_fields.append(col)\n\nprint(f\"\\nNested fields detected: {articles_nested_fields}\")\n\n# Flatten if needed\nif articles_nested_fields:\n    print(f\"\\nFlattening {len(articles_nested_fields)} nested fields...\")\n    articles_flat = flatten_nested_fields(articles_df, articles_nested_fields)\n    print(f\"✓ Flattened DataFrame shape: {articles_flat.shape}\")\nelse:\n    articles_flat = articles_df\n    print(\"\\nNo nested fields to flatten\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Missing Values Analysis - Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Missing values analysis on FLATTENED articles DataFrame\nmissing_articles = pd.DataFrame({\n    'Column': articles_flat.columns,\n    'Missing_Count': articles_flat.isnull().sum(),\n    'Missing_Percentage': (articles_flat.isnull().sum() / len(articles_flat) * 100).round(2),\n    'Data_Type': articles_flat.dtypes\n}).sort_values('Missing_Percentage', ascending=False)\n\nprint(\"Missing Values Analysis - Articles (with nested fields flattened):\")\nprint(\"=\"*70)\ndisplay(missing_articles)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Sentiment Score Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze sentiment scores (check for flattened sentiment fields)\nsentiment_col = None\n\n# Try to find sentiment score column (may be flattened)\nfor col in articles_flat.columns:\n    if 'sentiment' in col.lower() and articles_flat[col].dtype in ['float64', 'int64']:\n        sentiment_col = col\n        break\n\nif sentiment_col:\n    print(f\"Sentiment column: {sentiment_col}\")\n    print(f\"\\nSentiment statistics:\")\n    print(articles_flat[sentiment_col].describe())\n    \n    # Visualize distribution\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    # Histogram\n    articles_flat[sentiment_col].hist(bins=50, ax=axes[0], edgecolor='black')\n    axes[0].set_xlabel('Sentiment Score')\n    axes[0].set_ylabel('Frequency')\n    axes[0].set_title('Sentiment Score Distribution')\n    axes[0].axvline(x=0, color='red', linestyle='--', label='Neutral')\n    axes[0].legend()\n    \n    # Box plot\n    articles_flat.boxplot(column=sentiment_col, ax=axes[1])\n    axes[1].set_ylabel('Sentiment Score')\n    axes[1].set_title('Sentiment Score Box Plot')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Sentiment classification\n    positive = (articles_flat[sentiment_col] > 0).sum()\n    negative = (articles_flat[sentiment_col] < 0).sum()\n    neutral = (articles_flat[sentiment_col] == 0).sum()\n    null_count = articles_flat[sentiment_col].isna().sum()\n    \n    print(f\"\\nSentiment distribution:\")\n    print(f\"  Positive: {positive} ({positive/len(articles_flat)*100:.1f}%)\")\n    print(f\"  Negative: {negative} ({negative/len(articles_flat)*100:.1f}%)\")\n    print(f\"  Neutral: {neutral} ({neutral/len(articles_flat)*100:.1f}%)\")\n    print(f\"  Missing: {null_count} ({null_count/len(articles_flat)*100:.1f}%)\")\nelse:\n    print(\"Warning: No numerical sentiment column found\")\n    print(\"\\nAvailable sentiment-related columns:\")\n    sentiment_cols = [col for col in articles_flat.columns if 'sentiment' in col.lower()]\n    for col in sentiment_cols:\n        print(f\"  {col}: {articles_flat[col].dtype}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Duplicate Articles Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check for duplicate articles (by title or URL) - using flattened DataFrame\ntitle_col = None\nfor col in ['title', 'headline', 'Title']:\n    if col in articles_flat.columns:\n        title_col = col\n        break\n\nif title_col:\n    duplicate_titles = articles_flat[articles_flat.duplicated(subset=[title_col], keep=False)]\n    print(f\"Duplicate articles (by {title_col}): {len(duplicate_titles)}\")\n    \n    if len(duplicate_titles) > 0:\n        cols_to_show = [title_col]\n        if 'ticker' in articles_flat.columns:\n            cols_to_show.append('ticker')\n        print(\"\\nSample duplicate titles:\")\n        display(duplicate_titles[cols_to_show].head(10))\nelse:\n    print(\"Warning: No title column found for duplicate detection\")\n\n# Check exact row duplicates\nexact_dup = articles_flat.duplicated().sum()\nprint(f\"\\nExact duplicate rows: {exact_dup}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cross-Collection Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check ticker consistency across collections (using flattened DataFrames)\ntickers_fundamentals = set(fundamentals_flat['ticker'].unique()) if 'ticker' in fundamentals_flat.columns else set()\ntickers_prices = set(stock_prices_df['ticker'].unique()) if 'ticker' in stock_prices_df.columns else set()\ntickers_articles = set(articles_flat['ticker'].unique()) if 'ticker' in articles_flat.columns else set()\n\nprint(\"Ticker Consistency Across Collections:\")\nprint(\"=\"*70)\nprint(f\"Unique tickers in fundamentals: {len(tickers_fundamentals)}\")\nprint(f\"Unique tickers in stock_prices: {len(tickers_prices)}\")\nprint(f\"Unique tickers in articles: {len(tickers_articles)}\")\n\n# Find tickers in fundamentals but not in stock_prices\nmissing_price_tickers = tickers_fundamentals - tickers_prices\nif missing_price_tickers:\n    print(f\"\\n⚠ Tickers in fundamentals but missing in stock_prices: {len(missing_price_tickers)}\")\n    print(f\"   {sorted(list(missing_price_tickers))[:10]}\")\n    if len(missing_price_tickers) > 10:\n        print(f\"   ... and {len(missing_price_tickers) - 10} more\")\n\n# Find tickers in fundamentals but not in articles\nmissing_articles_tickers = tickers_fundamentals - tickers_articles\nif missing_articles_tickers:\n    print(f\"\\n⚠ Tickers in fundamentals but missing in articles: {len(missing_articles_tickers)}\")\n    print(f\"   {sorted(list(missing_articles_tickers))[:10]}\")\n    if len(missing_articles_tickers) > 10:\n        print(f\"   ... and {len(missing_articles_tickers) - 10} more\")\n\n# Find common tickers across all collections\ncommon_tickers = tickers_fundamentals & tickers_prices & tickers_articles\nprint(f\"\\n✓ Tickers present in all three collections: {len(common_tickers)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Quality Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate comprehensive data quality report (using flattened DataFrames)\nprint(\"=\"*70)\nprint(\"DATA QUALITY SUMMARY REPORT\")\nprint(\"=\"*70)\n\nprint(\"\\n1. FUNDAMENTALS COLLECTION\")\nprint(\"-\" * 50)\nprint(f\"   Total records: {len(fundamentals_flat)}\")\nprint(f\"   Total columns (flattened): {len(fundamentals_flat.columns)}\")\nprint(f\"   Columns with 100% missing: {len(missing_analysis[missing_analysis['Missing_Percentage'] == 100])}\")\nprint(f\"   Columns with >50% missing: {len(missing_analysis[missing_analysis['Missing_Percentage'] > 50])}\")\nprint(f\"   Duplicate rows: {fundamentals_flat.duplicated().sum()}\")\n\nprint(\"\\n2. STOCK PRICES COLLECTION\")\nprint(\"-\" * 50)\nprint(f\"   Total records: {len(stock_prices_df)}\")\nprint(f\"   Unique tickers: {stock_prices_df['ticker'].nunique() if 'ticker' in stock_prices_df.columns else 'N/A'}\")\nif date_col:\n    print(f\"   Date range: {stock_prices_df[date_col].min()} to {stock_prices_df[date_col].max()}\")\nprint(f\"   Duplicate rows: {stock_prices_df.duplicated().sum()}\")\n\nprint(\"\\n3. ARTICLES COLLECTION\")\nprint(\"-\" * 50)\nprint(f\"   Total records: {len(articles_flat)}\")\nprint(f\"   Total columns (flattened): {len(articles_flat.columns)}\")\nprint(f\"   Unique tickers: {articles_flat['ticker'].nunique() if 'ticker' in articles_flat.columns else 'N/A'}\")\nif title_col:\n    print(f\"   Duplicate titles: {articles_flat.duplicated(subset=[title_col]).sum()}\")\nprint(f\"   Columns with missing data: {len(missing_articles[missing_articles['Missing_Count'] > 0])}\")\n\nprint(\"\\n4. CROSS-COLLECTION INTEGRITY\")\nprint(\"-\" * 50)\nprint(f\"   Tickers in all collections: {len(common_tickers)}\")\nif len(tickers_fundamentals) > 0:\n    print(f\"   Data coverage: {len(common_tickers) / len(tickers_fundamentals) * 100:.1f}%\")\n\nprint(\"\\n\" + \"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Inspection Results\n",
    "\n",
    "**STOP HERE** - Review the inspection results above before proceeding to data cleaning.\n",
    "\n",
    "Please report back on:\n",
    "1. Which columns with high missing percentages should be kept/removed?\n",
    "2. How should we handle duplicate records?\n",
    "3. Any specific data quality issues that need attention?\n",
    "\n",
    "After your review, we'll proceed with the data cleaning implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Export data quality reports to CSV for detailed review\noutput_dir = project_root / 'evaluation' / 'data_quality_reports'\noutput_dir.mkdir(exist_ok=True)\n\ntimestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n\n# Export missing values analysis\nmissing_analysis.to_csv(output_dir / f'fundamentals_missing_{timestamp}.csv', index=False)\nmissing_prices.to_csv(output_dir / f'stock_prices_missing_{timestamp}.csv', index=False)\nmissing_articles.to_csv(output_dir / f'articles_missing_{timestamp}.csv', index=False)\n\nprint(f\"Data quality reports exported to: {output_dir}\")\nprint(\"\\n✓ Inspection phase complete. Proceeding to data cleaning...\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}