{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b2356f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment variables loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Load environment variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a59a2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest date: 2025-12-02\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "# Connect to MongoDB\n",
    "username = quote_plus(\"Wrynaft\")\n",
    "password = quote_plus(\"Ryan@120104\")\n",
    "\n",
    "client = MongoClient(f\"mongodb+srv://{username}:{password}@cluster0.bjjt9fa.mongodb.net/?appName=Cluster0\")\n",
    "db = client['roundtable_ai']\n",
    "col = db['stock_prices']\n",
    "\n",
    "latest = col.find_one(\n",
    "    {},                     # no filter\n",
    "    sort=[(\"date\", -1)],    # sort by date descending\n",
    "    projection={\"date\": 1, \"_id\": 0}\n",
    ")\n",
    "\n",
    "print(\"Latest date:\", latest[\"date\"] if latest else \"No data found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3131d592",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8535841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool, ToolRuntime\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from pymongo import MongoClient\n",
    "from urllib.parse import quote_plus\n",
    "import numpy as np\n",
    "from functools import lru_cache\n",
    "import yfinance as yf\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def load_company_mapping():\n",
    "    df = pd.read_csv(\"ticker_list.csv\")\n",
    "    df['company_normalized'] = df['company_name'].str.lower().str.strip()\n",
    "    df['ticker_normalized'] = df['ticker'].str.upper().str.strip()\n",
    "    return df\n",
    "\n",
    "@tool\n",
    "def get_stock_data(ticker_symbol: str, period: str = \"1y\") -> str:\n",
    "    \"\"\"Fetches the latest stock data for the given ticker symbol from Yahoo Finance.\n",
    "    Returns a CSV string of the stock data.\"\"\"\n",
    "    try:\n",
    "        # Fixed current date\n",
    "        current_date = datetime(2025, 2, 12)\n",
    "        # Connect to MongoDB\n",
    "        username = quote_plus(\"Wrynaft\")\n",
    "        password = quote_plus(\"Ryan@120104\")\n",
    "\n",
    "        client = MongoClient(f\"mongodb+srv://{username}:{password}@cluster0.bjjt9fa.mongodb.net/?appName=Cluster0\")\n",
    "        db = client['roundtable_ai']\n",
    "        col = db['stock_prices']\n",
    "\n",
    "        period_map = {\n",
    "            \"1y\": 365,\n",
    "            \"6mo\": 180,\n",
    "            \"3mo\": 90,\n",
    "            \"1mo\": 30,\n",
    "            \"5y\": 365*5\n",
    "        }\n",
    "\n",
    "        days = period_map.get(period, 365)   # default to 1 year\n",
    "        start_date = current_date - timedelta(days=days)\n",
    "\n",
    "        # Query MongoDB\n",
    "        cursor = col.find(\n",
    "            {\n",
    "                \"ticker\": ticker_symbol,\n",
    "                \"date\": {\"$gte\": start_date}\n",
    "            },\n",
    "            {\n",
    "                \"_id\": 0,       # remove MongoDB internal ID\n",
    "                \"ticker\": 1,\n",
    "                \"date\": 1,\n",
    "                \"open\": 1,\n",
    "                \"high\": 1,\n",
    "                \"low\": 1,\n",
    "                \"close\": 1,\n",
    "                \"volume\": 1\n",
    "            }\n",
    "        ).sort(\"date\", 1)\n",
    "\n",
    "        df = pd.DataFrame(list(cursor))\n",
    "\n",
    "        if df.empty:\n",
    "            return f\"No data found for ticker symbol: {ticker_symbol}\"\n",
    "\n",
    "        # Convert datetime to string for CSV output\n",
    "        df[\"date\"] = df[\"date\"].dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        return df.to_csv(index=False)\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching stock data for {ticker_symbol}: {str(e)}\"\n",
    "        \n",
    "\n",
    "@tool\n",
    "def analyze_stock_metrics(ticker_symbol: str, csv_string: str, risk_free_rate: float = 0.05) -> str:\n",
    "    \"\"\"Analyzes stock metrics such as volatility and risk metrics.\"\"\"\n",
    "    df = pd.read_csv(StringIO(csv_string), index_col=0, parse_dates=True)\n",
    "    daily_returns = df[\"close\"].pct_change().dropna()\n",
    "    if len(daily_returns) < 2:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": \"Insufficient data for calculations\",\n",
    "        }\n",
    "    mean_daily_return = daily_returns.mean()\n",
    "    daily_volatility = daily_returns.std()\n",
    "\n",
    "    # Calculate cumulative return for proper annualized return\n",
    "    start_price = df[\"close\"].iloc[0]\n",
    "    end_price = df[\"close\"].iloc[-1]\n",
    "    cumulative_return = end_price / start_price - 1\n",
    "    trading_days = len(df)\n",
    "\n",
    "    # Annualized metrics\n",
    "    annualized_return = (1 + cumulative_return) ** (252 / trading_days) - 1\n",
    "    annualized_volatility = daily_volatility * np.sqrt(252)\n",
    "\n",
    "    # Sharpe ratio\n",
    "    sharpe_ratio = (annualized_return - risk_free_rate) / annualized_volatility if annualized_volatility > 0 else 0\n",
    "\n",
    "    # Maximum drawdown calculation\n",
    "    cumulative_returns = (1 + daily_returns).cumprod()\n",
    "    rolling_max = cumulative_returns.expanding().max()\n",
    "    drawdowns = (cumulative_returns - rolling_max) / rolling_max\n",
    "    max_drawdown = drawdowns.min()\n",
    "\n",
    "    # Value at Risk (VaR) - 5% and 1%\n",
    "    var_5 = np.percentile(daily_returns, 5)\n",
    "    var_1 = np.percentile(daily_returns, 1)\n",
    "\n",
    "    # Additional statistics\n",
    "    skewness = daily_returns.skew()\n",
    "    kurtosis = daily_returns.kurtosis()\n",
    "\n",
    "    # Price performance metrics\n",
    "    total_return = (df['close'].iloc[-1] - df['close'].iloc[0]) / df['close'].iloc[0]\n",
    "    \n",
    "    result = {\n",
    "        \"success\": True,\n",
    "        \"symbol\": ticker_symbol.upper(),\n",
    "        \"analysis_period\": {\n",
    "            \"start_date\": df.index[0].strftime(\"%Y-%m-%d\"),\n",
    "            \"end_date\": df.index[-1].strftime(\"%Y-%m-%d\"),\n",
    "            \"trading_days\": len(df)\n",
    "        },\n",
    "        \"price_metrics\": {\n",
    "            \"start_price\": float(df['Close'].iloc[0]),\n",
    "            \"end_price\": float(df['Close'].iloc[-1]),\n",
    "            \"total_return\": float(total_return),\n",
    "            \"annualized_return\": float(annualized_return)\n",
    "        },\n",
    "        \"volatility_metrics\": {\n",
    "            \"daily_volatility\": float(daily_volatility),\n",
    "            \"annualized_volatility\": float(annualized_volatility),\n",
    "            \"volatility_percentage\": float(annualized_volatility * 100)\n",
    "        },\n",
    "        \"risk_metrics\": {\n",
    "            \"sharpe_ratio\": float(sharpe_ratio),\n",
    "            \"max_drawdown\": float(max_drawdown),\n",
    "            \"max_drawdown_percentage\": float(max_drawdown * 100),\n",
    "            \"var_5_percent\": float(var_5),\n",
    "            \"var_1_percent\": float(var_1),\n",
    "            \"risk_free_rate\": float(risk_free_rate)\n",
    "        },\n",
    "        \"distribution_metrics\": {\n",
    "            \"mean_daily_return\": float(mean_daily_return),\n",
    "            \"skewness\": float(skewness),\n",
    "            \"kurtosis\": float(kurtosis),\n",
    "            \"positive_days\": int((daily_returns > 0).sum()),\n",
    "            \"negative_days\": int((daily_returns < 0).sum())\n",
    "        },\n",
    "        \"volume_metrics\": {\n",
    "            \"average_volume\": float(df['Volume'].mean()),\n",
    "            \"volume_volatility\": float(df['Volume'].std()),\n",
    "            \"latest_volume\": float(df['Volume'].iloc[-1])\n",
    "        }\n",
    "    }\n",
    "    return result\n",
    "\n",
    "@tool\n",
    "def resolve_ticker_symbol(query: str) -> str:\n",
    "    \"\"\"Resolves a company name to its ticker symbol.\"\"\"\n",
    "    # Check if it's already a ticker (all caps, 1-5 characters)\n",
    "    df = load_company_mapping()\n",
    "\n",
    "    query_norm = query.lower().strip()\n",
    "    exact_match = df[df['company_normalized'] == query_norm]\n",
    "    if not exact_match.empty:\n",
    "        row = exact_match.iloc[0]\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"query\": query,\n",
    "            \"ticker\": row['ticker_normalized'],\n",
    "            \"company_name\": row['company_name'],\n",
    "            \"resolution_method\": \"exact_match\"\n",
    "        }\n",
    "    \n",
    "    ticker_match = df[df['ticker_normalized'] == query.upper().strip()]\n",
    "    if not ticker_match.empty:\n",
    "        row = ticker_match.iloc[0]\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"query\": query,\n",
    "            \"ticker\": row['ticker_normalized'],\n",
    "            \"company_name\": row['company_name'],\n",
    "            \"resolution_method\": \"ticker_match\"\n",
    "        }\n",
    "    \n",
    "    partial_matches = df[df['company_normalized'].str.contains(query_norm)]\n",
    "    if len(partial_matches) == 1:\n",
    "        row = partial_matches.iloc[0]\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"query\": query,\n",
    "            \"ticker\": row['ticker_normalized'],\n",
    "            \"company_name\": row['company_name'],\n",
    "            \"resolution_method\": \"partial_match\"\n",
    "        }\n",
    "    elif len(partial_matches) > 1:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"resolution_method\": \"multiple_partial_matches\",\n",
    "            \"query\": query,\n",
    "            \"candidates\": [\n",
    "                {\n",
    "                    \"ticker\": row['ticker_normalized'],\n",
    "                    \"company_name\": row['company_name']\n",
    "                } for _, row in partial_matches.iterrows()\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    all_companies = df['company_name'].tolist()\n",
    "    best_match, score, idx = process.extractOne(query, all_companies, scorer=fuzz.WRatio, score_cutoff=70)\n",
    "    if best_match:\n",
    "        row = df.iloc[idx]\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"query\": query,\n",
    "            \"ticker\": row['ticker_normalized'],\n",
    "            \"company_name\": row['company_name'],\n",
    "            \"resolution_method\": \"fuzzy_match\",\n",
    "            \"confidence\": score\n",
    "        }\n",
    "    # Fallback method\n",
    "    try:\n",
    "        potential_ticker = query.upper().strip()\n",
    "        info = yf.Ticker(potential_ticker).info  # Will raise error if invalid\n",
    "        if info and 'symbol' in info:\n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"query\": query,\n",
    "                \"ticker\": potential_ticker,\n",
    "                \"company_name\": info.get('longName', 'Unknown'),\n",
    "                \"resolution_method\": \"yfinance_lookup\"\n",
    "            }\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return {\n",
    "        \"success\": False,\n",
    "        \"query\": query,\n",
    "        \"error\": f\"Could not resolve '{query}' to ticker symbol.\",\n",
    "        \"suggestions\": [\n",
    "            \"Try using the stock ticker directly (e.g. 1155.KL for Maybank)\",\n",
    "            \"Check spelling of the company name\",\n",
    "            \"Use the full official company name like 'Malayan Banking Berhad'\"\n",
    "        ]\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785a8c51",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef19d05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "model_name = \"meta-llama/Llama-3-8b-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    ")\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=2048)\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba9aa1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"As a valuation equity analyst, your primary responsibility is to analyze the valuation trends of a given asset or \n",
    "         portfolio over an extended time horizon. To complete the task, you must analyze the historical valuation data of the\n",
    "         asset or portfolio provided, identify trends and patterns in valuation metrics over time, and interpret the implications\n",
    "        of these trends for investors or stakeholders.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daf0f137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9428fa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "checkpointer = InMemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29c0066c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'type': 'text', 'text': \"I can only provide a comprehensive analysis and recommendation if you provide the historical valuation data for the 1155.KL stock in a CSV format. This data will allow me to identify trends and patterns in valuation metrics over time.\\n\\nAdditionally, if you have a specific risk-free rate in mind, please provide it. This will enable a more precise analysis of the stock's risk metrics.\", 'extras': {'signature': 'CocIAXLI2nzDr3qVmqNMze0Lffz4Xs/5aqgxopItVpsldPnxy5e9eioO5wDoGxWOJYjNSwnZjtaIRKJL3IS7HpTkc0wKpXF+SDkLlJsyy8RT8ccfYinlL3SNeiDDhJCvL6/f50p8vuUgu85/6iR26irk54Tgy7KgiBZVm8OSinvy8ru7OJ/sr+vkYK+hIRnOSOZTXsQ2HryDMO+LP495la4clSidsUsaHtC8+7qeKm6zzNFtdb3e6lQx1H/OytvC7eVpIoq9Xywt96Xx4IhGtMPTvXhoVIJp5QL02S1Kqn2NTLdMAeoqxIcxOZf9/EK3jXL7xQ43w4WE+bBmHUvxUrLg+mDokU0epebFn9UPMf16BCn7wFKoVxa5bF2UybP/MjunpcFicVGQodvhiMjd1LeLgkVrBIZCbkXXTHFEplgBGhSjgT2NzGdO8H7h0+v7Kn2Is7qRN+thlXdtgAFwE1ogGCWZyJ/AbC2URzmP0UUMch7IaYAMebTUoZOu17PLH2aHv78Gs7TkF6Kuhcw3YY2UzGVgCtXp+NoxWidEiy/8aV8OSh42NuNVCSoRDSq+spniZboUXcs9dK6wuU2vjxwfI62ejfhFtxyJrybUyyQnIE/hNmBvw0rRPyMgi3RhArqR2ppJCB6c7E9azh407kyQmnB2fZrZHpKvhPoV8CWz01fwR3ejH9IrumXJV0cP/C0r4/wF9bAkDSuCTfiAcWo1/BNttc2b2Bg001aJo/z5GOd/JJmiBPUb9PBw1TUY/1TRvnEG5er8RG0HIJWJBdV2S/62tkClw5muQ2dAblpQjA1O1+xveaDGujZhrS7803+pDQWM3v1T/gcYgYh89cAxp9heHRLlIx8e6U4iVHJ8qV4KyzZK8qlhqEEEiXywMtAQ+AohC+ZK5muLH7N1JpdHBZZ2kMQS/czthNoWnVjCNyo7QjMCdCC8OAaCbeHjGovVkBuTCPKO+CVN3kvx8xtkkQJl6wx0pJwnoykst2mYZsLYeoEkdY/iXwHPIzdB4Dt5ib9KvvrGUOoKQ8xUpd87NVCWFUjLk95jLOhi+V0sBhIEXi929AC7XlUuJtGF2nI9yGXCOM9q9g9s5RWsbqmW63Oa3fSxn/PJbE0BM+kVRwkdzKxIdcBA15uRMmeqXaBhc8/OU6Ja7TR8aFqh3sWpp9zugGb27Vy/UWzrbkVaeybQNn3OcAsHSeFAUqbbPDXsnaKplcJgMib5sKuVN6AG3POLC8w0s8/w+db+bFnN+6h9fDXV+NwpfrcZ5445jkNTfvJTjFh+JCZzbB+UtGIu5WWNB6ZCuSksZKv54hF5BXFhW3lfzVR/fnsjMSLjk3/+N4dtNotdKkU6CQxI0ORluB/rvji/WHc='}}]\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    tools=[analyze_stock_metrics],\n",
    "    checkpointer=checkpointer,\n",
    ")\n",
    "\n",
    "# `thread_id` is a unique identifier for a given conversation.\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Provide analysis and recommendation whether a risk neutral investor should invest 1155.KL stock with only two option either BUY or SELL, with no option of HOLD.\"}]},\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(response[\"messages\"][-1].content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32cc7c54",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmessages\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SystemMessage, trim_messages\n\u001b[32m      3\u001b[39m trimmer = trim_messages(\n\u001b[32m      4\u001b[39m     max_tokens=\u001b[32m65\u001b[39m,\n\u001b[32m      5\u001b[39m     strategy=\u001b[33m\"\u001b[39m\u001b[33mlast\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     token_counter=\u001b[43mmodel\u001b[49m,\n\u001b[32m      7\u001b[39m     include_system=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      8\u001b[39m     allow_partial=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m      9\u001b[39m     start_on=\u001b[33m\"\u001b[39m\u001b[33mhuman\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=65,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d032503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessageState, StateGraph\n",
    "\n",
    "workflow = StateGraph(state_schema=MessageState)\n",
    "\n",
    "def call_model(state: MessageState):\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    print(f\"Messages after trimming: {len(trimmed_messages)}\")\n",
    "    print(\"Remaining messages:\")\n",
    "    for msg in trimmed_messages:\n",
    "        print(f\"  {type(msg).__name__}: {msg.content}\")\n",
    "    prompt = prompt_template.invoke(\n",
    "        {\"messages\": trimmed_messages, \"language\": state[\"language\"]}\n",
    "    )\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"message\": response}\n",
    "\n",
    "workflow.add(START, \"model\")\n",
    "workflow.add(\"model\", call_model)\n",
    "\n",
    "memory_saver = MemorySaver()\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "app = workflow.compile(checkpointer=memory_saver)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
